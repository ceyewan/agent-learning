from fastmcp import FastMCP, Context
import asyncio
from datetime import datetime
import json

mcp = FastMCP("Enhanced Sampling Demo")


@mcp.tool()
async def analyze_sentiment_with_summary(text: str, ctx: Context) -> str:
    """
    åˆ†ææ–‡æœ¬æƒ…æ„Ÿå¹¶æä¾›è¯¦ç»†æ‘˜è¦

    è¿™ä¸ªå·¥å…·ä¼šè¿›è¡Œä¸¤æ¬¡ç‹¬ç«‹çš„é‡‡æ ·è°ƒç”¨ï¼š
    1. æƒ…æ„Ÿåˆ†æé‡‡æ ·
    2. æ–‡æœ¬æ‘˜è¦é‡‡æ ·

    æ¯æ¬¡é‡‡æ ·éƒ½æ˜¯å®Œå…¨ç‹¬ç«‹çš„ï¼Œä¸ä¼šå…±äº«ä¸Šä¸‹æ–‡ã€‚
    """

    start_time = datetime.now()
    await ctx.info(f"ğŸš€ å¼€å§‹åˆ†ææ–‡æœ¬ | é•¿åº¦: {len(text)} å­—ç¬¦ | æ—¶é—´: {start_time.strftime('%H:%M:%S')}")

    if len(text.strip()) == 0:
        await ctx.error("âŒ è¾“å…¥æ–‡æœ¬ä¸ºç©º")
        return "é”™è¯¯ï¼šè¾“å…¥æ–‡æœ¬ä¸èƒ½ä¸ºç©º"

    try:
        # ==================== ç¬¬ä¸€æ¬¡ç‹¬ç«‹é‡‡æ ·ï¼šæƒ…æ„Ÿåˆ†æ ====================
        await ctx.info("ğŸ“Š å¼€å§‹ç¬¬ä¸€æ¬¡é‡‡æ ·ï¼šæƒ…æ„Ÿåˆ†æ")
        await ctx.debug(f"é‡‡æ ·å‚æ•° - æ¸©åº¦: 0.3, æœ€å¤§ä»¤ç‰Œ: 500, æ¨¡å‹åå¥½: qwen-turbo")

        sentiment_start = datetime.now()
        sentiment_response = await ctx.sample(
            messages=f"è¯·åˆ†æä»¥ä¸‹æ–‡æœ¬çš„æƒ…æ„Ÿå€¾å‘ï¼š\n\n{text}",
            system_prompt="""
                ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æƒ…æ„Ÿåˆ†æä¸“å®¶ã€‚è¯·æŒ‰ç…§ä»¥ä¸‹æ ¼å¼åˆ†ææ–‡æœ¬æƒ…æ„Ÿï¼š

                1. æƒ…æ„Ÿåˆ†ç±»ï¼šæ­£é¢/è´Ÿé¢/ä¸­æ€§
                2. ç½®ä¿¡åº¦ï¼š0-100%
                3. å…³é”®æƒ…æ„Ÿè¯æ±‡ï¼šåˆ—å‡º3-5ä¸ªå…³é”®è¯
                4. æƒ…æ„Ÿå¼ºåº¦ï¼šä½/ä¸­/é«˜
                5. ç®€è¦è§£é‡Šï¼šä¸€å¥è¯è¯´æ˜åˆ¤æ–­ä¾æ®

                è¯·ä¿æŒåˆ†æå®¢è§‚å‡†ç¡®ã€‚""",
            temperature=0.3,
            max_tokens=500,
            model_preferences=["openai/qwen-turbo-latest"]
        )

        sentiment_duration = (datetime.now() - sentiment_start).total_seconds()
        await ctx.info(f"âœ… ç¬¬ä¸€æ¬¡é‡‡æ ·å®Œæˆ | è€—æ—¶: {sentiment_duration:.2f}ç§’")

        # ç­‰å¾…ä¸€ç§’ï¼Œè®©æ—¥å¿—è¾“å‡ºæ›´æ¸…æ™°
        await asyncio.sleep(1)

        # ==================== ç¬¬äºŒæ¬¡ç‹¬ç«‹é‡‡æ ·ï¼šæ–‡æœ¬æ‘˜è¦ ====================
        await ctx.info("ğŸ“ å¼€å§‹ç¬¬äºŒæ¬¡é‡‡æ ·ï¼šæ–‡æœ¬æ‘˜è¦")
        await ctx.debug(f"é‡‡æ ·å‚æ•° - æ¸©åº¦: 0.7, æœ€å¤§ä»¤ç‰Œ: 800, æ— æ¨¡å‹åå¥½")

        summary_start = datetime.now()
        summary_response = await ctx.sample(
            messages=f"è¯·ä¸ºä»¥ä¸‹æ–‡æœ¬ç”Ÿæˆè¯¦ç»†æ‘˜è¦ï¼š\n\n{text}",
            system_prompt="""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æœ¬æ‘˜è¦ä¸“å®¶ã€‚è¯·ç”Ÿæˆç»“æ„åŒ–çš„æ‘˜è¦ï¼š

                1. æ ¸å¿ƒä¸»é¢˜ï¼šç”¨ä¸€å¥è¯æ¦‚æ‹¬ä¸»è¦å†…å®¹
                2. å…³é”®ä¿¡æ¯ï¼šåˆ—å‡º3-5ä¸ªè¦ç‚¹
                3. æ–‡æœ¬ç‰¹ç‚¹ï¼šåˆ†æå†™ä½œé£æ ¼å’Œè¯­è¨€ç‰¹è‰²
                4. ç›®æ ‡å—ä¼—ï¼šæ¨æµ‹å¯èƒ½çš„è¯»è€…ç¾¤ä½“
                5. æ€»ç»“ï¼šç”¨2-3å¥è¯è¿›è¡Œæ•´ä½“æ€»ç»“

                è¯·ä¿æŒæ‘˜è¦å…¨é¢è€Œç®€æ´ã€‚""",
            temperature=0.7,
            max_tokens=800
        )

        summary_duration = (datetime.now() - summary_start).total_seconds()
        await ctx.info(f"âœ… ç¬¬äºŒæ¬¡é‡‡æ ·å®Œæˆ | è€—æ—¶: {summary_duration:.2f}ç§’")

        # ==================== æ•´åˆç»“æœ ====================
        total_duration = (datetime.now() - start_time).total_seconds()
        await ctx.info(f"ğŸ‰ åˆ†æå®Œæˆ | æ€»è€—æ—¶: {total_duration:.2f}ç§’")

        # æ ¼å¼åŒ–æœ€ç»ˆç»“æœ
        result = f"""
# ğŸ“Š æ–‡æœ¬åˆ†ææŠ¥å‘Š

## ğŸ“ˆ æƒ…æ„Ÿåˆ†æç»“æœ
{getattr(sentiment_response, 'text', str(sentiment_response))}

## ğŸ“‹ æ–‡æœ¬æ‘˜è¦ç»“æœ  
{getattr(summary_response, 'text', str(summary_response))}

---
**åˆ†æç»Ÿè®¡**
- åŸæ–‡é•¿åº¦: {len(text)} å­—ç¬¦
- æƒ…æ„Ÿåˆ†æè€—æ—¶: {sentiment_duration:.2f}ç§’
- æ‘˜è¦ç”Ÿæˆè€—æ—¶: {summary_duration:.2f}ç§’
- æ€»å¤„ç†æ—¶é—´: {total_duration:.2f}ç§’
- åˆ†ææ—¶é—´: {start_time.strftime('%Y-%m-%d %H:%M:%S')}
"""

        return result.strip()

    except Exception as e:
        error_msg = f"é‡‡æ ·è¿‡ç¨‹å‘ç”Ÿé”™è¯¯: {str(e)}"
        await ctx.error(f"âŒ {error_msg}")

        # æä¾›é™çº§æœåŠ¡
        fallback_result = f"""
# âš ï¸ åˆ†æå¤±è´¥ï¼Œæä¾›åŸºç¡€ä¿¡æ¯

**æ–‡æœ¬åŸºç¡€ç»Ÿè®¡ï¼š**
- å­—ç¬¦æ•°: {len(text)}
- å•è¯æ•°: {len(text.split())}
- æ®µè½æ•°: {len(text.split('\\n\\n'))}
- åŒ…å«æ„Ÿå¹å·: {'æ˜¯' if '!' in text else 'å¦'}
- åŒ…å«é—®å·: {'æ˜¯' if '?' in text else 'å¦'}

**é”™è¯¯ä¿¡æ¯:** {error_msg}
**å»ºè®®:** è¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–ç¨åé‡è¯•
"""
        return fallback_result.strip()


@mcp.tool()
async def analyze_with_context_continuity(text: str, ctx: Context) -> str:
    """
    å¸¦ä¸Šä¸‹æ–‡è¿ç»­æ€§çš„åˆ†æå·¥å…·

    æ¼”ç¤ºå¦‚ä½•åœ¨ç¬¬äºŒæ¬¡é‡‡æ ·ä¸­åŒ…å«ç¬¬ä¸€æ¬¡é‡‡æ ·çš„ç»“æœï¼Œ
    å®ç°ä¸Šä¸‹æ–‡çš„è¿ç»­æ€§ã€‚
    """

    await ctx.info("ğŸ”— å¼€å§‹å¸¦ä¸Šä¸‹æ–‡è¿ç»­æ€§çš„åˆ†æ")

    try:
        # ç¬¬ä¸€æ¬¡é‡‡æ ·ï¼šæƒ…æ„Ÿåˆ†æ
        await ctx.info("1ï¸âƒ£ æ‰§è¡Œæƒ…æ„Ÿåˆ†æ")
        sentiment_response = await ctx.sample(
            messages=f"åˆ†ææƒ…æ„Ÿï¼š{text}",
            system_prompt="ä½ æ˜¯æƒ…æ„Ÿåˆ†æä¸“å®¶ï¼Œç®€æ´å›ç­”æƒ…æ„Ÿç±»å‹å’Œç½®ä¿¡åº¦ã€‚",
            temperature=0.3,
            max_tokens=200
        )

        sentiment_result = getattr(
            sentiment_response, 'text', str(sentiment_response))
        await ctx.debug(f"æƒ…æ„Ÿåˆ†æç»“æœ: {sentiment_result[:50]}...")

        # ç¬¬äºŒæ¬¡é‡‡æ ·ï¼šåŸºäºæƒ…æ„Ÿåˆ†æç»“æœç”Ÿæˆæ‘˜è¦
        await ctx.info("2ï¸âƒ£ åŸºäºæƒ…æ„Ÿåˆ†æç”Ÿæˆæ‘˜è¦")
        contextual_summary_response = await ctx.sample(
            messages=[
                f"åŸå§‹æ–‡æœ¬: {text}",
                f"æƒ…æ„Ÿåˆ†æç»“æœ: {sentiment_result}",
                "è¯·åŸºäºä¸Šè¿°æƒ…æ„Ÿåˆ†æç»“æœï¼Œç”Ÿæˆä¸€ä¸ªè€ƒè™‘äº†æƒ…æ„Ÿè‰²å½©çš„è¯¦ç»†æ‘˜è¦ã€‚"
            ],
            system_prompt="ä½ æ˜¯æ‘˜è¦ä¸“å®¶ï¼Œèƒ½å¤Ÿç»“åˆæƒ…æ„Ÿåˆ†æç»“æœç”Ÿæˆæ›´å‡†ç¡®çš„æ‘˜è¦ã€‚",
            temperature=0.6,
            max_tokens=600
        )

        summary_result = getattr(
            contextual_summary_response, 'text', str(contextual_summary_response))

        result = f"""
# ğŸ”— ä¸Šä¸‹æ–‡è¿ç»­æ€§åˆ†ææŠ¥å‘Š

## ğŸ¯ æƒ…æ„Ÿåˆ†æï¼ˆç¬¬ä¸€æ­¥ï¼‰
{sentiment_result}

## ğŸ“ åŸºäºæƒ…æ„Ÿçš„æ‘˜è¦ï¼ˆç¬¬äºŒæ­¥ï¼ŒåŒ…å«ç¬¬ä¸€æ­¥ç»“æœï¼‰
{summary_result}

---
**è¯´æ˜ï¼š** ç¬¬äºŒæ¬¡é‡‡æ ·æ˜ç¡®åŒ…å«äº†ç¬¬ä¸€æ¬¡é‡‡æ ·çš„ç»“æœï¼Œå®ç°äº†ä¸Šä¸‹æ–‡è¿ç»­æ€§ã€‚
"""

        await ctx.info("âœ… ä¸Šä¸‹æ–‡è¿ç»­æ€§åˆ†æå®Œæˆ")
        return result.strip()

    except Exception as e:
        await ctx.error(f"ä¸Šä¸‹æ–‡è¿ç»­æ€§åˆ†æå¤±è´¥: {str(e)}")
        return f"åˆ†æå¤±è´¥: {str(e)}"


@mcp.tool()
async def simple_echo_test(message: str, ctx: Context) -> str:
    """ç®€å•çš„å›æ˜¾æµ‹è¯•å·¥å…·ï¼Œç”¨äºéªŒè¯åŸºç¡€åŠŸèƒ½"""

    await ctx.info(f"ğŸ”„ æ‰§è¡Œå›æ˜¾æµ‹è¯•: {message[:30]}...")

    try:
        response = await ctx.sample(
            messages=f"è¯·ç®€å•å›å¤è¿™æ¡æ¶ˆæ¯ï¼š{message}",
            system_prompt="ä½ æ˜¯ä¸€ä¸ªå‹å¥½çš„åŠ©æ‰‹ï¼Œè¯·ç®€æ´åœ°å›å¤ç”¨æˆ·çš„æ¶ˆæ¯ã€‚",
            temperature=0.5,
            max_tokens=100
        )

        result = getattr(response, 'text', str(response))
        await ctx.info("âœ… å›æ˜¾æµ‹è¯•å®Œæˆ")

        return f"åŸå§‹æ¶ˆæ¯: {message}\nå›å¤: {result}"

    except Exception as e:
        await ctx.error(f"å›æ˜¾æµ‹è¯•å¤±è´¥: {str(e)}")
        return f"å›æ˜¾å¤±è´¥: {str(e)}"


if __name__ == "__main__":
    print("ğŸš€ å¯åŠ¨ Enhanced MCP æœåŠ¡å™¨...")
    print("ğŸ“¡ ç›‘å¬ç«¯å£: 8080")
    print("ğŸ”— è¿æ¥æ–¹å¼: SSE")
    print("=" * 50)
    mcp.run("sse", port=8080)
